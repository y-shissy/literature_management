import streamlit as st
from pydrive.drive import GoogleDrive
import tempfile
import shutil
import os
from llama_index.core import StorageContext, load_index_from_storage
import openai

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(layout="wide")

# Google Driveæ¥ç¶š
drive = st.session_state.get("drive")

# OpenAI APIã‚­ãƒ¼ã®è¨­å®š
openai.api_key = st.secrets["openai_api_key"]

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¤‰æ•°ã‚’åˆæœŸåŒ–
if "loaded_indices" not in st.session_state:
    st.session_state["loaded_indices"] = []

def load_indices_from_drive():
    """Google Driveã‹ã‚‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹"""
    file_list = drive.ListFile({'q': "title contains '_index.zip' and trashed=false"}).GetList()
    if not file_list:
        st.error("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒGoogle Driveä¸Šã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return []

    indices = []
    for index_file in file_list:
        try:
            zip_file_path = os.path.join(tempfile.gettempdir(), index_file['title'])
            index_file.GetContentFile(zip_file_path)
            extract_dir = tempfile.mkdtemp()
            shutil.unpack_archive(zip_file_path, extract_dir)

            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã‚€
            storage_context = StorageContext.from_defaults(persist_dir=extract_dir)
            index = load_index_from_storage(storage_context)

            # èª­ã¿è¾¼ã‚“ã ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«è¿½åŠ 
            indices.append({"name": index_file["title"].replace("_index.zip", ""), "index": index})
            os.remove(zip_file_path)
        except Exception as e:
            st.error(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ« {index_file['title']} ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

    return indices

def query_all_indices(prompt, indices):
    """å…¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’çµ±åˆã—ã¦æ¤œç´¢"""
    combined_results = []
    for item in indices:
        try:
            query_engine = item["index"].as_query_engine()
            result = query_engine.query(prompt)
            combined_results.append({
                "source": item["name"],
                "content": result.response,
                "metadata": result.metadata,
            })
        except Exception as e:
            st.error(f"{item['name']} ã§ã®ã‚¯ã‚¨ãƒªå®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
    return combined_results

def format_results(results):
    """çµæœã‚’æ•´å½¢ã—ã¦è¡¨ç¤º"""
    # é–¢é€£æ€§ã®é«˜ã„é †ã«ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½5ä»¶ã®ã¿å–å¾—
    sorted_results = sorted(results, key=lambda x: len(x["content"].get("response", "")), reverse=True)[:5]

    st.subheader("ğŸ“Œ æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„çµæœ")
    for res in sorted_results:
        response_text = res["content"].get("response", "å†…å®¹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        metadata = res.get("metadata", {})
        source = res["source"]

        st.markdown(f"### æ–‡çŒ®å: {source}")
        st.markdown(f"**å›ç­”å†…å®¹:**\n{response_text}")

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°è¡¨ç¤º
        if metadata:
            page_label = metadata.get("page_label", "ä¸æ˜")
            file_name = metadata.get("file_name", "ä¸æ˜")
            st.markdown(f"**å‚ç…§å…ƒãƒšãƒ¼ã‚¸:** {page_label}")
            st.markdown(f"**ãƒ•ã‚¡ã‚¤ãƒ«å:** {file_name}")

    if len(results) > 5:
        with st.expander("ğŸ“š ä»–ã®é–¢é€£æ–‡çŒ®ã‚’è¦‹ã‚‹"):
            for res in results[5:]:
                response_text = res["content"].get("response", "å†…å®¹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                metadata = res.get("metadata", {})
                source = res["source"]

                st.markdown(f"### æ–‡çŒ®å: {source}")
                st.markdown(f"**å›ç­”å†…å®¹:**\n{response_text}")

                if metadata:
                    page_label = metadata.get("page_label", "ä¸æ˜")
                    file_name = metadata.get("file_name", "ä¸æ˜")
                    st.markdown(f"**å‚ç…§å…ƒãƒšãƒ¼ã‚¸:** {page_label}")
                    st.markdown(f"**ãƒ•ã‚¡ã‚¤ãƒ«å:** {file_name}")

def main():
    st.title(":robot_face: AI Chat")
    st.markdown("### æ–‡çŒ®PDFæƒ…å ±ã‹ã‚‰æƒ…å ±ã‚’æ¤œç´¢")

    if not st.session_state["loaded_indices"]:
        with st.spinner("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™..."):
            st.session_state["loaded_indices"] = load_indices_from_drive()

    if not st.session_state["loaded_indices"]:
        st.error("æœ‰åŠ¹ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    indices = st.session_state["loaded_indices"]

    if st.button("ãƒªã‚»ãƒƒãƒˆ", use_container_width=True):
        st.session_state.messages = [{"role": "assistant", "content": "è³ªå•ã‚’ã©ã†ã"}]
        st.experimental_rerun()

    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "è³ªå•ã‚’ã©ã†ã"}]

    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.write(msg["content"])

    if prompt_input := st.chat_input():
        prompt = prompt_input + "\nã“ã®è³ªå•ã‚’æ—¥æœ¬èªã¨è‹±èªã®ä¸¡æ–¹ã§æ¤œç´¢ã—ã€æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„çµæœã‚’æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"
        responses = query_all_indices(prompt, indices)

        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)

        if responses:
            st.session_state.messages.append({"role": "assistant", "content": "æ¤œç´¢çµæœã‚’è¡¨ç¤ºä¸­..."})
            format_results(responses)

if __name__ == "__main__":
    main()
